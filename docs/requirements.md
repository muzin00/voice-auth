# 要件定義書

> VoiceAuth - 音声認証サーバー

## 概要

音声によるランダムプロンプト認証システム。ユーザーが事前に登録した「数字（0〜9）」の音声データに基づき、認証時にランダムに指定される数字列（例：「4326」）を発声させることで、**「本人であるか（声紋）」** と **「正しい言葉を話しているか（ASR検証）」** を同時に検証する。

推論エンジンには `sherpa-onnx` を採用し、一般的な CPU 環境での低遅延動作を実現する。

## 認証方式

| 認証要素 | 種類                     | 判定基準                     |
| -------- | ------------------------ | ---------------------------- |
| 声紋     | 生体認証（What you are） | 類似度がしきい値以上         |
| 発話内容 | 知識認証（What you say） | ASRテキストがプロンプトと一致 |

### 認証ロジック

```
認証成功条件 = ASR一致 AND 声紋類似度 ≧ しきい値（0.75）
```

## 技術スタック

| コンポーネント       | 技術                        |
| -------------------- | --------------------------- |
| フレームワーク       | FastAPI                     |
| 通信方式             | WebSocket（登録・認証）|
| 音声区間検出 (VAD)   | sherpa-onnx (Silero VAD)    |
| 音声認識 (ASR)       | sherpa-onnx (SenseVoice)    |
| 声紋抽出 (SV)        | sherpa-onnx (CAM++)         |
| 音声変換             | PyAV（webm → wav）          |
| データベース         | PostgreSQL / SQLite         |
| 処理方式             | 同期処理                    |
| ハードウェア要件     | CPU（AVX命令セット対応推奨）|

### モデル役割

- **Silero VAD（VAD）**: 人の声が含まれる区間を検出、無音/環境音を除去
- **SenseVoice（ASR）**: 発話内容のテキスト化、および各数字の発話タイミング（タイムスタンプ）の取得
- **CAM++（SV）**: 切り出された音声区間からの特徴ベクトル抽出（192次元）

## 認証フロー

```
プロンプト生成（例：「4326」）
       ↓
ユーザー音声入力
       ↓
┌──────────────────────────────────────┐
│  1. VAD検出（sherpa-onnx Silero VAD） │
│     → 人の声が含まれる区間を検出     │
│     → 無音/環境音なら即時リトライ    │
└──────────────────────────────────────┘
       ↓
┌──────────────────────────────────────┐
│  2. ASR検証（sherpa-onnx SenseVoice） │
│     → 発話内容をテキスト化           │
│     → プロンプトと一致するか判定     │
│     → 不一致なら即時NG               │
│     → 各数字のタイムスタンプ取得     │
└──────────────────────────────────────┘
       ↓
┌──────────────────────────────────────┐
│  3. 声紋検証（sherpa-onnx CAM++）     │
│     → タイムスタンプで音声を分割     │
│     → 各数字の声紋ベクトル抽出       │
│     → 登録済みベクトルと類似度計算   │
│     → 平均スコア算出                 │
└──────────────────────────────────────┘
       ↓
┌──────────────────────────────────────┐
│  4. 認証判定                          │
│     └─ 平均類似度 ≧ 0.75 で成功      │
└──────────────────────────────────────┘
       ↓
   認証結果（成功 / 失敗）
```

## 機能要件

### 登録フェーズ（Enrollment）

ユーザーごとに「0〜9」の数字を**4桁×5セット（計20文字）**で発話させ、各数字2サンプルの平均ベクトルを登録する。バックアップ用PINも同時に登録する。

| ID     | 機能                     | 説明                                                              |
| ------ | ------------------------ | ----------------------------------------------------------------- |
| FR-1-1 | バランスド・プロンプト生成 | 0〜9が**各2回ずつ**出現する4桁×5セットを生成。同じ数字が連続しないこと（例: 4326, 8105, ...）|
| FR-1-2 | 厳格なASR検証            | 認識テキスト≠プロンプトなら**即時破棄してリトライ**                |
| FR-1-3 | セグメンテーション       | SenseVoiceのタイムスタンプで連続発話を個別数字に切り出し           |
| FR-1-4 | ベクトル化               | 切り出した区間ごとにCAM++で声紋ベクトル（192次元）を抽出           |
| FR-1-5 | 重心計算（Centroid）     | 各数字2サンプルの**平均ベクトル**をマスター登録ベクトルとする      |
| FR-1-6 | PIN登録                  | 4桁数字をUI入力、**SHA-256ハッシュ化**して保存（バックアップ認証用）|
| FR-1-7 | 保存                     | speaker_idに紐づけて数字別ベクトル（計10個）とPINハッシュを保存    |

#### 登録処理フロー

```
1. プロンプト生成
   → ["4326", "8105", "9718", "5029", "3674"] (0〜9が各2回)

2. 5セット分ループ (i = 0 to 4):
   ┌─────────────────────────────────────────┐
   │  While True (リトライループ):            │
   │    音声入力 → SenseVoice ASR            │
   │    IF 認識テキスト ≠ プロンプト:         │
   │      → エラー表示、再録音               │
   │    ELSE:                                │
   │      → タイムスタンプで4分割            │
   │      → 各断片をCAM++でベクトル化        │
   │      → temp_vectors[digit].append()    │
   │      → Break (次のセットへ)             │
   └─────────────────────────────────────────┘

3. 重心計算:
   FOR digit in 0〜9:
     final_vector[digit] = mean(temp_vectors[digit])  # 2サンプル平均

4. PIN登録:
   → UIでPIN入力 → SHA-256ハッシュ化

5. DB保存:
   → speaker_id, final_vectors, hashed_pin を保存
```

#### 登録時の制約

| 項目           | 制約                                                         |
| -------------- | ------------------------------------------------------------ |
| リトライ上限   | 1プロンプトに対し5回連続失敗で警告、フローやり直し           |
| 録音時間       | 1秒未満 or 10秒以上はエラー                                  |
| 無音検知       | VAD有効化、無音ファイルはASRに渡さない                       |

### 認証フェーズ（Authentication）

ランダムな数字列を提示し、リアルタイムで検証を行う。

| ID     | 機能           | 説明                                                          |
| ------ | -------------- | ------------------------------------------------------------- |
| FR-2-1 | プロンプト生成 | ランダムな数字列（4〜6桁）を生成・提示                         |
| FR-2-2 | 音声取得       | マイクまたはファイルからユーザー音声を取得                     |
| FR-2-3 | ASR検証        | SenseVoiceでテキスト化し、プロンプトと一致するか判定           |
| FR-2-4 | タイムスタンプ | ASRで各数字の発話タイミングを取得                              |
| FR-2-5 | 音声分割       | タイムスタンプに基づき各数字の音声区間を切り出し               |
| FR-2-6 | 声紋照合       | 切り出した音声と登録済みベクトルのコサイン類似度を計算         |
| FR-2-7 | スコア算出     | 全数字の類似度から平均スコアを算出                             |
| FR-2-8 | 認証判定       | ASR一致 AND 平均スコア ≧ 0.75 で成功                           |

### その他機能

| ID   | 機能     | 説明                                   |
| ---- | -------- | -------------------------------------- |
| FR-4 | デモ画面 | 音声登録・認証のデモUI（1画面）        |

## エッジケースと対策

| 項目           | 課題                                                    | 対策                                                                    |
| -------------- | ------------------------------------------------------- | ----------------------------------------------------------------------- |
| 読みの揺れ     | 「0」を「ゼロ/レイ/マル」、「7」を「ナナ/シチ」と読む   | 正規化ロジック：ASR結果を数字記号に変換する辞書マッピングを実装         |
| 無音・ノイズ   | 発話の前後に長い無音や環境音が入る                      | sherpa-onnx内蔵のVAD（Voice Activity Detection）を有効にし発話区間のみ処理 |
| 早口・遅口     | ユーザーによって話す速度が違う                          | SenseVoiceのタイムスタンプ機能により動的に区間を切り出すため自動対応    |
| 声紋不一致     | ASRは正解したが声紋スコアが低い                         | なりすまし（録音再生など）の可能性が高いため認証失敗とする              |

## 依存パッケージ

```
sherpa-onnx
soundfile
numpy
av          # PyAV: webm → wav 変換
websockets  # WebSocket通信
```

## 非機能要件

| 項目       | 要件                         |
| ---------- | ---------------------------- |
| スケール   | 個人利用（スケール要件なし） |
| 処理方式   | 同期処理                     |
| レスポンス | 数秒以内                     |
| 対応言語   | 日本語                       |

## セキュリティ要件：音声データの取り扱い

### 基本方針

**本番運用では音声ファイル（WAV等）の保存は禁止。ベクトル（数値データ）のみ保存する。**

これは現代の生体認証システムの鉄則である。

### 音声ファイルを保存してはいけない理由

| リスク | 説明 |
|--------|------|
| 生体情報の不可逆性 | パスワードは変更できるが、漏洩した声は「変更」できない。一生涯なりすましリスクに晒される |
| ディープフェイクの素材 | 「0〜9のクリアな音声」はAI音声合成の最高級素材。攻撃者に詐欺の材料を提供することになる |
| パスワードハッシュと同じ | 音声ファイル＝平文パスワード（危険）、ベクトル＝ハッシュ（元の声には戻せないため安全） |

### データ処理ライフサイクル

```python
def register_voice(user_id, audio_data):
    # 1. 音声からベクトルを抽出
    vector = extract_vector(audio_data)

    # 2. ベクトルのみをDBに保存
    save_vector_to_db(user_id, vector)

    # 3. 生の音声データは明示的に破棄
    del audio_data
    # 一時ファイルがあれば必ず削除
    if os.path.exists("temp.wav"):
        os.remove("temp.wav")
```

### 例外：保存が許可されるケース

**開発中のデバッグ・精度チューニング期間のみ**

| ケース | ルール |
|--------|--------|
| 開発テスト | 開発者自身の声でテスト中は保存OK |
| 本番運用 | 「ログ保存機能：OFF」に設定 |
| 本番ログ必要時 | ユーザー規約に明記 + 24時間以内の自動削除を実装 |

### セキュリティ上の利点

万が一ハッキングされても、流出するのは「謎の数字の羅列（ベクトル）」だけであり、ユーザーの声そのものは守られる。

---

## 関連ドキュメント

- [アーキテクチャガイド](./architecture-guide.md) - 設計・ディレクトリ構成・データモデル
- [API仕様書](./api-specification.md) - WebSocket/REST API詳細
- [デプロイガイド](./deployment.md) - GCP Cloud Runへのデプロイ
